{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def load_json_data(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "train_annotations = load_json_data('train_mini_arthropoda.json')\n",
    "val_annotations = load_json_data('val_arthropoda.json')\n",
    "\n",
    "def get_image_paths_and_labels(annotations):\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    for image_info in annotations['images']:\n",
    "        image_path = f\"{image_info['file_name']}\"  # 根据实际存储路径调整\n",
    "        image_paths.append(image_path)\n",
    "        # 找到该图像的所有标注\n",
    "        image_annotations = [ann for ann in annotations['annotations'] if ann['image_id'] == image_info['id']]\n",
    "        # 假设每个图像只有一个类别标签\n",
    "        if image_annotations:\n",
    "            labels.append(image_annotations[0]['category_id'])\n",
    "    return image_paths, labels\n",
    "\n",
    "train_image_paths, train_labels = get_image_paths_and_labels(train_annotations)\n",
    "val_image_paths, val_labels = get_image_paths_and_labels(val_annotations)\n",
    "\n",
    "# 创建一个 DataFrame\n",
    "train_data = pd.DataFrame({\n",
    "    'filename': train_image_paths,\n",
    "    'label': train_labels\n",
    "})\n",
    "\n",
    "val_data = pd.DataFrame({\n",
    "    'filename': val_image_paths,\n",
    "    'label': val_labels\n",
    "})\n",
    "\n",
    "train_data.to_csv('train_data.csv', index=False)\n",
    "val_data.to_csv('val_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train_data = pd.read_csv('train_data.csv')\n",
    "val_data = pd.read_csv('val_data.csv')\n",
    "\n",
    "train_data['label'] = train_data['label'].astype(str)\n",
    "val_data['label'] = val_data['label'].astype(str)\n",
    "\n",
    "unique_labels = np.unique(train_data['label'])\n",
    "print('唯一标签总数', len(unique_labels))\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "# data generator\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    dataframe=train_data,\n",
    "    directory='d:/BrainBoner/inatDetection/', \n",
    "    x_col='filename',  # DataFrame中包含文件名的列名\n",
    "    y_col='label',  # DataFrame中包含标签的列名\n",
    "    target_size=(224, 224),  # 将图像大小调整为224x224\n",
    "    batch_size=16,\n",
    "    class_mode='categorical'  # 类别模式\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "val_generator = val_datagen.flow_from_dataframe(\n",
    "    dataframe=val_data,\n",
    "    directory='d:/BrainBoner/inatDetection/',  # 同上\n",
    "    x_col='filename',\n",
    "    y_col='label',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=16,\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取训练集的类索引\n",
    "train_class_indices = train_generator.class_indices\n",
    "# 获取验证集的类索引\n",
    "val_class_indices = val_generator.class_indices\n",
    "\n",
    "# 保存训练集和验证集的类索引到文件\n",
    "pd.DataFrame.from_dict(train_class_indices, orient='index').to_csv('class_indices.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 获取一批图像数据\n",
    "sample_training_images, _ = next(train_generator)\n",
    "\n",
    "# 定义函数来显示图像\n",
    "def plotImages(images_arr):\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(20,20))\n",
    "    axes = axes.flatten()\n",
    "    for img, ax in zip(images_arr, axes):\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 显示图像\n",
    "plotImages(sample_training_images[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import EfficientNetV2B0\n",
    "from keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from keras.optimizers import SGD\n",
    "from keras.models import Model\n",
    "from keras.regularizers import l2\n",
    "\n",
    "base_model = EfficientNetV2B0(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(2752, activation='softmax', kernel_regularizer=l2(0.01))(x)\n",
    "model = Model(inputs=base_model.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend\n",
    "\n",
    "def smooth_labels_loss(label_smoothing=0.1):\n",
    "    def loss(y_true, y_pred):\n",
    "        num_classes = backend.int_shape(y_pred)[-1]\n",
    "        smooth = label_smoothing / num_classes\n",
    "        y_true = y_true * (1 - label_smoothing) + smooth\n",
    "        return backend.categorical_crossentropy(y_true, y_pred)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=SGD(learning_rate=0.01, momentum=0.9, nesterov=True), loss=smooth_labels_loss(0.1), metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath='ckpt.h5', monitor='val_loss', save_best_only=True, save_weights_only=False)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, min_lr=0.00001, verbose=1)\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    epochs=10,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=len(val_generator),\n",
    "    verbose=1,\n",
    "    callbacks=[checkpoint, reduce_lr]\n",
    ")\n",
    "\n",
    "model.save(\"full_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "def smooth_labels_loss(label_smoothing=0.1):\n",
    "    def loss(y_true, y_pred):\n",
    "        num_classes = backend.int_shape(y_pred)[-1]\n",
    "        smooth = label_smoothing / num_classes\n",
    "        y_true = y_true * (1 - label_smoothing) + smooth\n",
    "        return backend.categorical_crossentropy(y_true, y_pred)\n",
    "    return loss\n",
    "\n",
    "modelContinue = load_model('ckpt.h5', custom_objects={'loss': smooth_labels_loss(0.1)})\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath='ckpt.h5', monitor='val_loss', save_best_only=True, save_weights_only=False)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, min_lr=0.00001, verbose=1)\n",
    "\n",
    "ContinuedHistory = modelContinue.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    epochs=100,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=len(val_generator),\n",
    "    verbose=1,\n",
    "    callbacks=[checkpoint, reduce_lr]\n",
    ")\n",
    "model.save(\"full_model.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dltf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
